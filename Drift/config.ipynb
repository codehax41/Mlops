{"cells":[{"cell_type":"code","source":["#Notebook used from\n#Data+AI Summit talk 2021 on Databricks\n#by \n#Chengyin Eng\n#Niall Turbitt"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84ff7ebe-3116-44fc-be66-bd6d1030ac15","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#All the imports\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom mlflow.exceptions import RestException\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import Schema, ColSpec\nfrom delta.tables import DeltaTable\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tempfile\nimport os\nimport numpy as np\nimport pandas as pd\nimport pyspark.sql.functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"d0694e8f-bf79-4065-b75d-ea9291bd2f25","inputWidgets":{},"title":"Imports"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"c79a7d39-0fe885d0efd33682ac9a6d14"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}}],"execution_count":0},{"cell_type":"code","source":["#%pip install protobuf==3.20.*"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bda021b7-359b-49bf-b310-cef7959f2e56","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#return any features that exceed the specified null threshold.\ndef check_null_proportion(new_pdf, null_proportion_threshold):\n  missing_stats = pd.DataFrame(new_pdf.isnull().sum() / len(new_pdf)).transpose()\n  null_dict = {}\n  null_col_list = missing_stats.columns[(missing_stats >= null_proportion_threshold).iloc[0]]\n  for feature in null_col_list:\n    null_dict[feature] = missing_stats[feature][0]\n  try:\n    assert len(null_dict) == 0\n  except:\n    print(\"Alert: There are feature(s) that exceed(s) the expected null threshold. Please ensure that the data is ingested correctly\")\n    print(null_dict)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"95eb4af4-3925-4759-934f-d83c7461f314","inputWidgets":{},"title":"Helper Functions"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def check_diff_in_summary_stats(new_stats_pdf, prod_stats_pdf, num_cols, stats_threshold_limit, statistic_list):\n  feature_diff_list = []\n  for feature in num_cols: \n    print(f\"\\nCHECKING {feature}.........\")\n    for statistic in statistic_list: \n      val = prod_stats_pdf[[str(feature)]].loc[str(statistic)][0]\n      upper_val_limit = val * (1 + stats_threshold_limit)\n      lower_val_limit = val * (1 - stats_threshold_limit)\n\n      new_metric_value = new_stats_pdf[[str(feature)]].loc[str(statistic)][0]\n\n      if new_metric_value < lower_val_limit:\n        feature_diff_list.append(str(feature))\n        print(f\"\\tThe {statistic} {feature} in the new data is at least {stats_threshold_limit *100}% lower than the {statistic} in the production data. Decreased from {round(val, 2)} to {round(new_metric_value,2)}.\")\n\n      elif new_metric_value > upper_val_limit:\n        feature_diff_list.append(str(feature))\n        print(f\"\\tThe {statistic} {feature} in the new data is at least {stats_threshold_limit *100}% higher than the {statistic} in the production data. Increased from {round(val, 2)} to {round(new_metric_value, 2)}.\")\n\n      else:\n        pass\n  \n  return np.unique(feature_diff_list)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcb3eae0-d447-49a1-9a5c-9db2940fb0a5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def check_diff_in_variances(reference_df, new_df, num_cols, p_threshold):\n  var_dict = {}\n  for feature in num_cols:\n    levene_stat, levene_pval = stats.levene(reference_df[str(feature)], new_df[str(feature)], center=\"median\")\n    if levene_pval <= p_threshold:\n      var_dict[str(feature)] = levene_pval\n  try:\n    assert len(var_dict) == 0\n    print(f\"No features have significantly different variances compared to production data at p-value {p_threshold}\")\n  except:\n    print(f\"The feature(s) below have significantly different variances compared to production data at p-value {p_threshold}\")\n    print(var_dict)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f24add08-e5f3-4c85-b4a4-ea64a6ba8ec7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#ks test: Kolmogorovâ€“Smirnov_test is used to check distributions of the two samples are the same or not\ndef check_dist_ks_bonferroni_test(reference_df, new_df, num_cols, p_threshold, ks_alternative=\"two-sided\"):\n    ks_dict = {}\n    ### Bonferroni correction \n    corrected_alpha = p_threshold / len(num_cols)\n    print(f\"The Bonferroni-corrected alpha level is {round(corrected_alpha, 4)}. Any features with KS statistic below this alpha level have shifted significantly.\")\n    for feature in num_cols:\n      ks_stat, ks_pval = stats.ks_2samp(reference_df[feature], new_df[feature], alternative=ks_alternative, mode=\"asymp\")\n      if ks_pval <= corrected_alpha:\n        ks_dict[feature] = ks_pval\n    try:\n      assert len(ks_dict) == 0\n      print(f\"No feature distributions has shifted according to the KS test at the Bonferroni-corrected alpha level of {round(corrected_alpha, 4)}. \")\n    except:\n      print(f\"The feature(s) below have significantly different distributions compared to production data at Bonferroni-corrected alpha level of {round(corrected_alpha, 4)}, according to the KS test\")\n      print(\"\\t\", ks_dict)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7e6d122-68cd-4650-8bcb-4d6cdbc81a02","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def check_categorical_diffs(reference_pdf, new_pdf, cat_cols, p_threshold):\n  chi_dict = {}\n  catdiff_list = []\n  \n  # Compute modes for all cat cols\n  reference_modes_pdf = reference_pdf[cat_cols].mode(axis=0, numeric_only=False, dropna=True)\n  new_modes_pdf = new_pdf[cat_cols].mode(axis=0, numeric_only=False, dropna=True)\n  \n  for feature in cat_cols: \n    prod_array = reference_pdf[feature].value_counts(ascending=True).to_numpy()\n    new_array = new_pdf[feature].value_counts(ascending=True).to_numpy()\n    try:\n      chi_stats, chi_pval = stats.chisquare(new_array, prod_array)\n      if chi_pval <= p_threshold:\n        chi_dict[feature] = chi_pval\n    except ValueError as ve :\n      catdiff_list.append(feature)\n      \n    # Check if the mode has changed\n    \n    reference_mode = reference_modes_pdf[feature].iloc[0]\n    new_mode = new_modes_pdf[feature].iloc[0]\n    try:\n      assert reference_mode == new_mode\n    except:\n      print(f\"The mode for {feature} has changed from {reference_mode} to {new_mode}.\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e04b38ad-13ba-4700-a9e0-014d58daf3be","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def compare_model_perfs(current_staging_run, current_prod_run, min_model_perf_threshold, metric_to_check):\n  model_diff_fraction = current_staging_run.data.metrics[str(metric_to_check)] / current_prod_run.data.metrics[str(metric_to_check)]\n  model_diff_percent = round((model_diff_fraction - 1)*100, 2)\n  print(f\"Staging run's {metric_to_check}: {round(current_staging_run.data.metrics[str(metric_to_check)],3)}\")\n  print(f\"Current production run's {metric_to_check}: {round(current_prod_run.data.metrics[str(metric_to_check)],3)}\")\n\n  if model_diff_percent >= 0 and (model_diff_fraction - 1 >= min_model_perf_threshold):\n    print(f\"The current staging run exceeds the model improvement threshold of at least +{min_model_perf_threshold}. You may proceed with transitioning the staging model to production now.\")\n    \n  elif model_diff_percent >= 0 and (model_diff_fraction - 1  < min_model_perf_threshold):\n    print(f\"CAUTION: The current staging run does not meet the improvement threshold of at least +{min_model_perf_threshold}. Transition the staging model to production with caution.\")\n  else: \n    print(f\"ALERT: The current staging run underperforms by {model_diff_percent}% when compared to the production model. Do not transition the staging model to production.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc2ca0df-8c42-4a1a-a265-0080c0a789c6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def plot_boxplots(unique_feature_diff_array, reference_pdf, new_pdf):\n  sns.set_theme(style=\"whitegrid\")\n  fig, ax = plt.subplots(len(unique_feature_diff_array), 2, figsize=(15,8))\n  fig.suptitle(\"Distribution Comparisons between Incoming Data and Production Data\")\n  ax[0, 0].set_title(\"Production Data\")\n  ax[0, 1].set_title(\"Incoming Data\")\n\n  for i in range(len(unique_feature_diff_array)):\n    p1 = sns.boxplot(ax=ax[i, 0], x=reference_pdf[unique_feature_diff_array[i]])\n    p1.set_xlabel(str(unique_feature_diff_array[i]))\n    p1.annotate(str(unique_feature_diff_array[i]), xy=(10,0.5))\n    p2 = sns.boxplot(ax=ax[i, 1], x=new_pdf[unique_feature_diff_array[i]])\n    p2.annotate(str(unique_feature_diff_array[i]), xy=(10,0.5))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a61852cd-5e52-400a-a663-4de285f7cffc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def cleanup_registered_model(registry_model_name):\n  client = MlflowClient()\n\n  filter_string = f'name=\"{registry_model_name}\"'\n\n  model_versions = client.search_model_versions(filter_string=filter_string)\n  \n  if len(model_versions) > 0:\n    print(f\"Deleting following registered model: {registry_model_name}\")\n    \n    # Move any versions of the model to Archived\n    for model_version in model_versions:\n      try:\n        model_version = client.transition_model_version_stage(name=model_version.name,\n                                                              version=model_version.version,\n                                                              stage=\"Archived\")\n      except mlflow.exceptions.RestException:\n        pass\n\n    client.delete_registered_model(registry_model_name)\n    \n  else:\n    print(\"No registered models to delete\")  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d8ed95a-5209-4615-9f54-eedc9f4c476f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_delta_version(delta_path):\n  delta_table = DeltaTable.forPath(spark, delta_path)\n  delta_table_history = delta_table.history() \n  delta_version = delta_table_history.first()[\"version\"]\n  \n  return delta_version"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"adbf9c98-1df0-4b97-bb2c-9d8901e04e33","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_summary_stats_pdf(pdf):\n  stats_pdf = pdf.describe(include=\"all\")\n  median_vals = pdf.median()\n  stats_pdf.loc[\"median\"] = median_vals\n  null_count = pdf.isna().sum()\n  stats_pdf.loc[\"null_count\"] = null_count\n\n  return stats_pdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c02092cf-fa6b-43ee-96fd-c79a1686a449","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def log_summary_stats_pdf_as_csv(pdf):\n  temp = tempfile.NamedTemporaryFile(prefix=\"summary_stats_\", suffix=\".csv\")\n  temp_name = temp.name\n  try:\n    pdf.to_csv(temp_name)\n    mlflow.log_artifact(temp_name, \"summary_stats.csv\")\n  finally:\n    temp.close() "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f1d8448-b860-4ded-b4f2-dd8d8b7af4e5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def load_summary_stats_pdf_from_run(run, local_tmp_dir):\n  client = MlflowClient()\n  if not os.path.exists(local_tmp_dir):\n      os.mkdir(local_tmp_dir)\n  local_path = client.download_artifacts(run.info.run_id, \"summary_stats.csv\", local_tmp_dir)\n  print(f\"Summary stats artifact downloaded in: {local_path}\")\n  \n  # Load the csv into a pandas DataFrame\n  summary_stats_path = local_path + \"/\" + os.listdir(local_path)[0]\n  summary_stats_pdf = pd.read_csv(summary_stats_path, index_col=\"Unnamed: 0\")\n  \n  return summary_stats_pdf "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e03dc594-a905-4ca0-aa85-5f7c3c376b71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def load_delta_table_from_run(run):\n  delta_path = run.data.params[\"delta_path\"]\n  delta_version = run.data.params[\"delta_version\"]\n  print(f\"Loading Delta table from path: {delta_path}; version: {delta_version}\")\n  df = spark.read.format(\"delta\").option(\"versionAsOf\", delta_version).load(delta_path)\n  \n  return df  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46461c76-b4e8-416a-9401-8a71de2318e3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def transition_model(model_version, stage):\n    client = MlflowClient()\n    \n    model_version = client.transition_model_version_stage(\n        name=model_version.name,\n        version=model_version.version,\n        stage=stage,\n        archive_existing_versions=True\n    )\n    return model_version "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"137499d8-86b5-49e8-bca2-139b24c3683b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def fetch_model_version(registry_model_name, stage=\"Staging\"):\n    client = MlflowClient()\n    filter_string = f'name=\"{registry_model_name}\"'\n    registered_model = client.search_registered_models(filter_string=filter_string)[0]\n\n    if len(registered_model.latest_versions) == 1:\n        model_version = registered_model.latest_versions[0]\n\n    else:\n        model_version = [model_version for model_version in registered_model.latest_versions if model_version.current_stage == stage][0]\n\n    return model_version"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ee66fe6-94d9-464a-80d0-d52a9ada8911","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_run_from_registered_model(registry_model_name, stage=\"Staging\"):\n    model_version = fetch_model_version(registry_model_name, stage)\n    run_id = model_version.run_id\n    run = mlflow.get_run(run_id)\n\n    return run  \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5c96934-8d89-4457-a5f2-568648a6266f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_sklearn_rf_pipeline(model_params, seed=42):\n  # Create pipeline component for numeric Features\n  numeric_transformer = Pipeline(steps=[\n      (\"imputer\", SimpleImputer(strategy='median'))])\n\n  # Create pipeline component for categorical Features\n  categorical_transformer = Pipeline(steps=[\n      (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n      (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n\n  # Combine numeric and categorical components into one preprocessor pipeline\n  # Use ColumnTransformer to apply the different preprocessing pipelines to different subsets of features\n  # Use selector (make_column_selector) to select which subset of features to apply pipeline to\n  preprocessor = ColumnTransformer(transformers=[\n      (\"numeric\", numeric_transformer, selector(dtype_exclude=\"category\")),\n      (\"categorical\", categorical_transformer, selector(dtype_include=\"category\"))\n  ])\n\n  pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor),\n                             (\"rf\", RandomForestRegressor(random_state=seed, \n                                                          **model_params))\n                            ])\n  \n  return pipeline\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81c41f80-3e48-463e-bc2c-72df63e354e8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def train_sklearn_rf_model(run_name, delta_path, model_params, misc_params, seed=42):\n  with mlflow.start_run(run_name=run_name) as run:\n\n    # Enable MLflow autologging\n    mlflow.autolog(log_input_examples=True, silent=True)\n    \n    # Load Delta table from delta_path\n    df = spark.read.format(\"delta\").load(delta_path)   \n    # Log Delta path and version\n    mlflow.log_param(\"delta_path\", delta_path)\n    delta_version = get_delta_version(delta_path)\n    mlflow.log_param(\"delta_version\", delta_version)\n    \n    # Track misc parameters used in pipeline creation (preprocessing) as json artifact\n    mlflow.log_dict(misc_params, \"preprocessing_params.json\")\n    target_col = misc_params[\"target_col\"]  \n    num_cols = misc_params[\"num_cols\"]    \n    cat_cols = misc_params[\"cat_cols\"]    \n\n    # Convert Spark DataFrame to pandas, as we will be training an sklearn model\n    pdf = df.toPandas() \n    # Convert all cat cols to category dtype\n    for c in cat_cols:\n        pdf[c] = pdf[c].astype(\"category\")    \n    \n    # Create summary statistics pandas DataFrame and log as a csv to MLflow\n    summary_stats_pdf = create_summary_stats_pdf(pdf)\n    log_summary_stats_pdf_as_csv(summary_stats_pdf)  \n    \n    # Track number of total instances and \"month\"\n    num_instances = pdf.shape[0]\n    mlflow.log_param(\"num_instances\", num_instances)  # Log number of instances\n    mlflow.log_param(\"month\", misc_params[\"month\"])   # Log month number\n    \n    # Split data\n    X = pdf.drop([misc_params[\"target_col\"], \"month\"], axis=1)\n    y = pdf[misc_params[\"target_col\"]]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    # Track train/test data info as params\n    num_training = X_train.shape[0]\n    mlflow.log_param(\"num_training_instances\", num_training)\n    num_test = X_test.shape[0]\n    mlflow.log_param(\"num_test_instances\", num_test)\n\n    # Fit sklearn pipeline with RandomForestRegressor model\n    rf_pipeline = create_sklearn_rf_pipeline(model_params)\n    rf_pipeline.fit(X_train, y_train)\n    # Specify data schema which the model will use as its ModelSignature\n    input_schema = Schema([\n      ColSpec(\"integer\", \"accommodates\"),\n      ColSpec(\"integer\", \"bedrooms\"),\n      ColSpec(\"integer\", \"beds\"),\n      ColSpec(\"integer\", \"number_of_reviews\"),\n      ColSpec(\"integer\", \"number_of_reviews_ltm\"),\n      ColSpec(\"integer\", \"minimum_nights\"),\n      ColSpec(\"integer\", \"review_scores_rating\"),\n      ColSpec(\"string\", \"host_is_superhost\"),\n      ColSpec(\"string\", \"neighbourhood_cleansed\"),\n      ColSpec(\"string\", \"property_type\"),\n      ColSpec(\"string\", \"room_type\")\n    ])\n    output_schema = Schema([ColSpec(\"double\")])\n    signature = ModelSignature(input_schema, output_schema)\n    mlflow.sklearn.log_model(rf_pipeline, \"model\", signature=signature)\n\n    # Evaluate the model\n    predictions = rf_pipeline.predict(X_test)\n    test_mse = mean_squared_error(y_test, predictions) \n    r2 = r2_score(y_test, predictions)\n    mlflow.log_metrics({\"test_mse\": test_mse,\n                       \"test_r2\": r2})\n\n  return run"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99255a00-a8ab-4811-b4d9-d1e545001d56","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c7e9c00-1a2e-4dcc-9a48-11d92570d4b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stats_threshold_limit =  0.05\np_threshold = 0.05\nmin_model_r2_threshold = 0.1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"4840ca6b-2590-4c76-ab3c-5647ab3735e7","inputWidgets":{},"title":"Variable Initialize"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# username\nusername = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply(\"user\")\nprint(username)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e6b9081-744d-409f-af26-4859cdc8b26d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"demo_yt_mlops@outlook.com\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["demo_yt_mlops@outlook.com\n"]}}],"execution_count":0},{"cell_type":"code","source":["workspace_project_home = f\"/Users/{username}/DT-2023\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eac0358a-eab3-4902-94c8-203336ce8b2a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["experiment_path = workspace_project_home + \"/airbnb_hawaii\"\nmlflow.set_experiment(experiment_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f1007de-c517-4239-8748-186ed93be69b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"2023/02/05 09:23:26 INFO mlflow.tracking.fluent: Experiment with name '/Users/demo_yt_mlops@outlook.com/DT-2023/airbnb_hawaii' does not exist. Creating a new experiment.\nOut[27]: <Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/4119769509188008', creation_time=1675589006729, experiment_id='4119769509188008', last_update_time=1675589006729, lifecycle_stage='active', name='/Users/demo_yt_mlops@outlook.com/DT-2023/airbnb_hawaii', tags={'mlflow.experiment.sourceName': '/Users/demo_yt_mlops@outlook.com/DT-2023/airbnb_hawaii',\n 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n 'mlflow.ownerEmail': 'demo_yt_mlops@outlook.com',\n 'mlflow.ownerId': '5792412687435845'}>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2023/02/05 09:23:26 INFO mlflow.tracking.fluent: Experiment with name '/Users/demo_yt_mlops@outlook.com/DT-2023/airbnb_hawaii' does not exist. Creating a new experiment.\nOut[27]: <Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/4119769509188008', creation_time=1675589006729, experiment_id='4119769509188008', last_update_time=1675589006729, lifecycle_stage='active', name='/Users/demo_yt_mlops@outlook.com/DT-2023/airbnb_hawaii', tags={'mlflow.experiment.sourceName': '/Users/demo_yt_mlops@outlook.com/DT-2023/airbnb_hawaii',\n 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n 'mlflow.ownerEmail': 'demo_yt_mlops@outlook.com',\n 'mlflow.ownerId': '5792412687435845'}>"]}}],"execution_count":0},{"cell_type":"code","source":["experiment_id = mlflow.get_experiment_by_name(experiment_path).experiment_id\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c14a4408-5f34-4654-910d-72b5848526c6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["experiment_id"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"767f17e4-5315-4f33-a324-d9f0da0b9375","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[29]: '4119769509188008'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[29]: '4119769509188008'"]}}],"execution_count":0},{"cell_type":"code","source":["#  MLflow Registry\nregistry_model_name = \"airbnb_hawaii\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ca8a773-35f1-40c1-8252-f141c37a9ea3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Set variables to use for reading/writing tmp artifacts and datasets\nproject_home_dir = f\"/Users/{username}/ram_drift/\"\nproject_local_tmp_dir = \"/dbfs\" + project_home_dir + \"tmp/\"\ndata_project_dir = f\"{project_home_dir}data/\"\n\n##################################################\nupload_data_loc = \"/FileStore/tables/temp_dir/airbnb-hawaii.delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9124a44c-9d67-4f27-a16a-40732f7feb5e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["raw_delta_path = \"/FileStore/tables/temp_dir/airbnb-hawaii.delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9be46dd8-9fa5-4f8a-b8d9-35f76081d8fb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Paths to write/read data from\nmonth_0_delta_path = data_project_dir + \"month_0_delta\"\n# Two separate Data paths - one with error data and one without the error\nmonth_1_error_delta_path = data_project_dir + \"month_1_error_delta\"\nmonth_1_fixed_delta_path = data_project_dir + \"month_1_fixed_delta\"\nmonth_2_delta_path = data_project_dir + \"month_2_delta\"\n\n##################################################\n\n# Define the path for table\ngold_delta_path = data_project_dir + \"airbnb_hawaii_delta\"\n\n# Ensure we start with no existing Delta table \ndbutils.fs.rm(gold_delta_path, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b802685-0576-40cd-8bbf-b98be3173c39","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[32]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[32]: True"]}}],"execution_count":0},{"cell_type":"code","source":["#For the very first run prepare the df and their columns\n\nairbnb_df = spark.read.format(\"delta\").load(upload_data_loc)\n\ntarget_col = \"price\"\nnum_cols = [\"accommodates\",\n            \"bedrooms\",\n            \"beds\",\n            \"minimum_nights\",\n            \"number_of_reviews\",\n            \"number_of_reviews_ltm\",\n            \"review_scores_rating\"]\ncat_cols = [\"host_is_superhost\",\n            \"neighbourhood_cleansed\",\n            \"property_type\",\n            \"room_type\"]\n\ncols_to_keep = [target_col] + num_cols + cat_cols\nairbnb_df = airbnb_df.select(cols_to_keep)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58b3ace4-6fdd-4668-91e0-b7386ba18d5b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["airbnb_df.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bad25b68-a33a-4f60-86d4-1b83567425e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----+------------+--------+----+--------------+-----------------+---------------------+--------------------+-----------------+----------------------+----------------+---------------+\n|price|accommodates|bedrooms|beds|minimum_nights|number_of_reviews|number_of_reviews_ltm|review_scores_rating|host_is_superhost|neighbourhood_cleansed|   property_type|      room_type|\n+-----+------------+--------+----+--------------+-----------------+---------------------+--------------------+-----------------+----------------------+----------------+---------------+\n|150.0|           2|       1|   1|             3|               11|                    2|                  88|                t|          South Kohala|Entire apartment|Entire home/apt|\n| 85.0|           2|       1|   1|             5|              168|                    2|                  93|                f|            South Kona|Entire apartment|Entire home/apt|\n+-----+------------+--------+----+--------------+-----------------+---------------------+--------------------+-----------------+----------------------+----------------+---------------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+--------+----+--------------+-----------------+---------------------+--------------------+-----------------+----------------------+----------------+---------------+\n|price|accommodates|bedrooms|beds|minimum_nights|number_of_reviews|number_of_reviews_ltm|review_scores_rating|host_is_superhost|neighbourhood_cleansed|   property_type|      room_type|\n+-----+------------+--------+----+--------------+-----------------+---------------------+--------------------+-----------------+----------------------+----------------+---------------+\n|150.0|           2|       1|   1|             3|               11|                    2|                  88|                t|          South Kohala|Entire apartment|Entire home/apt|\n| 85.0|           2|       1|   1|             5|              168|                    2|                  93|                f|            South Kona|Entire apartment|Entire home/apt|\n+-----+------------+--------+----+--------------+-----------------+---------------------+--------------------+-----------------+----------------------+----------------+---------------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df_0, df_1, df_2 = airbnb_df.randomSplit(weights=[1.0, 1.0, 1.0], seed=42)\n\ndf_0.write.format(\"delta\").mode(\"overwrite\").save(month_0_delta_path)\ndf_1.write.format(\"delta\").mode(\"overwrite\").save(month_1_fixed_delta_path)\n\n\n# Create a DataFrame which takes the clean data and introduces simulated errors into the dataset\ndf_1_err = (df_1\n             .withColumn(\"neighbourhood_cleansed\",                                 # Simulate some neighbourhood entires as being cleansed incorrectly  \n                         F.when((F.col(\"neighbourhood_cleansed\") == \"Primary Urban Center\") |  \n                                (F.col(\"neighbourhood_cleansed\") == \"Kihei-Makena\") | \n                                (F.col(\"neighbourhood_cleansed\") == \"Lahaina\") | \n                                (F.col(\"neighbourhood_cleansed\") == \"North Kona\"), F.lit(None)).otherwise(F.col(\"neighbourhood_cleansed\")))\n            .fillna(0, subset=[\"review_scores_rating\"])                            # Fill missing ratings with 0\n            .withColumn(\"review_scores_rating\", F.col(\"review_scores_rating\")/20)  # Scale ratings to be between 0 and 5\n            )\n\ndf_1_err.write.format(\"delta\").mode(\"overwrite\").save(month_1_error_delta_path)\n\n\ndf_2_err = df_2.withColumn(\"price\", F.col(\"price\") + (2*F.col(\"price\")*F.rand(seed=42)))\ndf_2_err.write.format(\"delta\").mode(\"overwrite\").save(month_2_delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7549feca-4dd8-4c3e-a0f5-e5d52c17f308","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82e4ddf7-a85b-4162-a779-8b569d61ebe3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"config","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4119769509187973}},"nbformat":4,"nbformat_minor":0}
